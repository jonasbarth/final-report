\relax 
\@nameuse{bbl@beforestart}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\citation{DBLP:journals/corr/MnihKSGAWR13}
\citation{alpha_go}
\citation{arulkumaran2019alphastar}
\citation{dubey2018investigating}
\citation{hessel2018rainbow}
\citation{bellemare2017distributional}
\citation{nachum2019does}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Motivation}{1}\protected@file@percent }
\citation{puterman2014markov}
\citation{franccois2018introduction}
\citation{franccois2018introduction}
\citation{watkins1992q}
\citation{bellman1966dynamic}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Objectives}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Literature Review}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Deep RL}{2}\protected@file@percent }
\newlabel{deep_rl}{{2.1}{2}}
\citation{huber1992robust}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Agent takes an action to move to the next state and observe a reward \cite {franccois2018introduction}\relax }}{3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:2}{{1}{3}}
\newlabel{eq:optimal_q}{{1}{3}}
\newlabel{eq:q_update}{{2}{3}}
\citation{lin1993reinforcement}
\citation{geirhos2018generalisation}
\citation{sutton1996generalization}
\citation{van2017hybrid}
\citation{cobbe2018quantifying}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Policy Network for Super Mario (not to scale)\relax }}{4}\protected@file@percent }
\newlabel{fig:2}{{2}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Target Networks}{4}\protected@file@percent }
\newlabel{eq:q_loss}{{3}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Experience Replay}{4}\protected@file@percent }
\citation{ribas2011neural}
\citation{ribas2011neural}
\citation{sutton1999between}
\citation{sutton1999between}
\citation{bacon2017option}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Hierarchical RL}{5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The Hierarchical RL Semi Markov Decision Process \cite {ribas2011neural}\relax }}{5}\protected@file@percent }
\newlabel{fig:hrl_mdp}{{3}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Option-Critic}{5}\protected@file@percent }
\citation{ribas2011neural}
\citation{ribas2011neural}
\citation{bacon2017option}
\citation{vezhnevets2017feudal}
\citation{dayan1993feudal}
\citation{vezhnevets2017feudal}
\citation{vezhnevets2017feudal}
\citation{DBLP:journals/corr/MnihKSGAWR13}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Option-Critic Architecture \cite {ribas2011neural}\relax }}{6}\protected@file@percent }
\newlabel{fig:4}{{4}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Feudal Networks (FuN)}{6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces FuN Architecure \cite {vezhnevets2017feudal}\relax }}{6}\protected@file@percent }
\newlabel{fig:4}{{5}{6}}
\citation{marioai}
\citation{jython}
\citation{jpype}
\citation{py4j}
\@writefile{toc}{\contentsline {section}{\numberline {3}System Requirements}{7}\protected@file@percent }
\citation{marioai}
\citation{siebert2001kobra}
\@writefile{toc}{\contentsline {section}{\numberline {4}System Implementation}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Overall Architecture}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Reward Function}{8}\protected@file@percent }
\newlabel{reward_function}{{4.2}{8}}
\citation{amodei2016concrete}
\citation{pypi}
\newlabel{fig:system_architecture}{{\caption@xref {fig:system_architecture}{ on input line 239}}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces System Architecture\relax }}{9}\protected@file@percent }
\citation{sutton2011reinforcement}
\citation{white2012bandit}
\citation{mnih2015human}
\citation{maroti2019rbed}
\citation{chuchro2017game}
\citation{gribbon2004novel}
\citation{DBLP:journals/corr/MnihKSGAWR13}
\citation{lample2017playing}
\citation{DBLP:journals/corr/MnihKSGAWR13}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Exploitation vs Exploration}{10}\protected@file@percent }
\newlabel{exploit_explore}{{4.3}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Variations of $\epsilon $ Decay\relax }}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Frame Preprocessing}{10}\protected@file@percent }
\newlabel{frame_preprocessing}{{4.4}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Frame Preprocessing Pipeline\relax }}{10}\protected@file@percent }
\citation{braylan2015frame}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Frame Stacking and Frame Skipping}{11}\protected@file@percent }
\newlabel{frame_stacking_skipping}{{4.5}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Stacked frames are given as input to the neural network. The dimension of the input thus becomes $84 \times 84 \times 3 \times 4$\relax }}{11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The agent selects action $a$ to be executed $k$ times in the emulator and observes the $k^{th}$ frame\relax }}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Custom Levels}{12}\protected@file@percent }
\newlabel{custom_levels}{{4.6}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Steps with Gap Level\relax }}{12}\protected@file@percent }
\newlabel{fig:steps_gap_level}{{11}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Two Pipes Level\relax }}{12}\protected@file@percent }
\newlabel{fig:}{{12}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Steps with Gap + Two Pipes\relax }}{12}\protected@file@percent }
\newlabel{fig:awesome_image3}{{13}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiment Design}{12}\protected@file@percent }
\citation{lecun2015deep}
\citation{ramachandran2017searching}
\citation{bergstra2012random}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Experiments carried out for the project. sequential indicates whether the model has been pretrained on another level already.\relax }}{13}\protected@file@percent }
\newlabel{tbl:experiments}{{1}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}DQN Agent}{13}\protected@file@percent }
\newlabel{dqn_experiment}{{6.1}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Illustration of the CNN used by the DQN Agent (not to scale). Stride is the number of pixels the convolutional filter moves. Kernel is the size of the filter.\relax }}{14}\protected@file@percent }
\newlabel{fig:dqn_cnn}{{14}{14}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces DQN Hyperparameters}}{14}\protected@file@percent }
\newlabel{tbl:dqn_hyperparams}{{2}{14}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Rewards accumulated by the DQN agent in each level. Final rewards are the total rewards acquired in the final episode of the run. Total rewards is the sum of all rewards of the run.\relax }}{14}\protected@file@percent }
\citation{bacon2017option}
\citation{oc_github}
\newlabel{fig:dqn_results_1}{{15a}{15}}
\newlabel{sub@fig:dqn_results_1}{{a}{15}}
\newlabel{fig:dqn_results_2}{{15b}{15}}
\newlabel{sub@fig:dqn_results_2}{{b}{15}}
\newlabel{fig:dqn_results_3}{{15c}{15}}
\newlabel{sub@fig:dqn_results_3}{{c}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces DQN Performance on the three Super Mario levels measured in the average reward per played episode.\relax }}{15}\protected@file@percent }
\newlabel{fig:dqn_results}{{15}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Option-Critic Agent}{15}\protected@file@percent }
\newlabel{oc_experiment}{{6.2}{15}}
\newlabel{fig:oc_results_1}{{16a}{15}}
\newlabel{sub@fig:oc_results_1}{{a}{15}}
\newlabel{fig:oc_results_2}{{16b}{15}}
\newlabel{sub@fig:oc_results_2}{{b}{15}}
\newlabel{fig:oc_results_3}{{16c}{15}}
\newlabel{sub@fig:oc_results_3}{{c}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Option Critic Performance on the three Super Mario levels measured in the average reward per played episode.\relax }}{15}\protected@file@percent }
\newlabel{fig:oc_results}{{16}{15}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The total sum of credits accumulated in each level by the Option Critic agent.\relax }}{15}\protected@file@percent }
\newlabel{tbl:oc_rewards}{{4}{15}}
\citation{fun_github}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}FuN Agent}{16}\protected@file@percent }
\newlabel{fun_experiment}{{6.3}{16}}
\newlabel{fig:fun_results_1}{{17a}{16}}
\newlabel{sub@fig:fun_results_1}{{a}{16}}
\newlabel{fig:fun_results_2}{{17b}{16}}
\newlabel{sub@fig:fun_results_2}{{b}{16}}
\newlabel{fig:fun_results_3}{{17c}{16}}
\newlabel{sub@fig:fun_results_3}{{c}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces FuN Performance on the three Super Mario levels measured in the average reward per played episode.\relax }}{16}\protected@file@percent }
\newlabel{fig:fun_results}{{17}{16}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Total rewards accumulated by the FuN agent in the final episode of the run and in the overall run.\relax }}{16}\protected@file@percent }
\citation{kirkpatrick2017overcoming}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Evaluation of Results}{17}\protected@file@percent }
\newlabel{fig:all_results_1}{{18a}{17}}
\newlabel{sub@fig:all_results_1}{{a}{17}}
\newlabel{fig:fun_results_2}{{18b}{17}}
\newlabel{sub@fig:fun_results_2}{{b}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Comparing the average rewards per episode for the two levels where models were pretrained.\relax }}{17}\protected@file@percent }
\newlabel{fig:all_results}{{18}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Further Policy Analysis}{18}\protected@file@percent }
\newlabel{fig:good_policy}{{19a}{18}}
\newlabel{sub@fig:good_policy}{{a}{18}}
\newlabel{fig:good_sequential_policy}{{19b}{18}}
\newlabel{sub@fig:good_sequential_policy}{{b}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces The average reward per run and the number of completed episodes. Pearson is a linear correlation coefficient. Spearman is a nonlinear correlation coefficient.\relax }}{18}\protected@file@percent }
\newlabel{fig:all_results}{{19}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Deviations From Original Specifications}{18}\protected@file@percent }
\citation{chuchro2017game}
\citation{song2016measuring}
\@writefile{toc}{\contentsline {section}{\numberline {8}Critical Assessment of Project}{19}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusion}{20}\protected@file@percent }
\bibstyle{ieeetran}
\bibdata{references}
\bibcite{DBLP:journals/corr/MnihKSGAWR13}{1}
\bibcite{alpha_go}{2}
\bibcite{arulkumaran2019alphastar}{3}
\bibcite{dubey2018investigating}{4}
\bibcite{hessel2018rainbow}{5}
\bibcite{bellemare2017distributional}{6}
\bibcite{nachum2019does}{7}
\bibcite{puterman2014markov}{8}
\bibcite{franccois2018introduction}{9}
\bibcite{watkins1992q}{10}
\bibcite{bellman1966dynamic}{11}
\bibcite{huber1992robust}{12}
\bibcite{lin1993reinforcement}{13}
\bibcite{geirhos2018generalisation}{14}
\bibcite{sutton1996generalization}{15}
\bibcite{van2017hybrid}{16}
\bibcite{cobbe2018quantifying}{17}
\bibcite{ribas2011neural}{18}
\bibcite{sutton1999between}{19}
\bibcite{bacon2017option}{20}
\bibcite{vezhnevets2017feudal}{21}
\bibcite{dayan1993feudal}{22}
\bibcite{marioai}{23}
\bibcite{jython}{24}
\bibcite{jpype}{25}
\bibcite{py4j}{26}
\bibcite{siebert2001kobra}{27}
\bibcite{amodei2016concrete}{28}
\bibcite{pypi}{29}
\bibcite{sutton2011reinforcement}{30}
\bibcite{white2012bandit}{31}
\bibcite{mnih2015human}{32}
\bibcite{maroti2019rbed}{33}
\bibcite{chuchro2017game}{34}
\bibcite{gribbon2004novel}{35}
\bibcite{lample2017playing}{36}
\bibcite{braylan2015frame}{37}
\bibcite{lecun2015deep}{38}
\bibcite{ramachandran2017searching}{39}
\bibcite{bergstra2012random}{40}
\bibcite{oc_github}{41}
\bibcite{fun_github}{42}
\bibcite{kirkpatrick2017overcoming}{43}
\bibcite{song2016measuring}{44}
