\relax 
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\citation{DBLP:journals/corr/MnihKSGAWR13}
\citation{alpha_go}
\citation{arulkumaran2019alphastar}
\citation{hessel2018rainbow}
\citation{bellemare2017distributional}
\citation{nachum2019does}
\citation{puterman2014markov}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Motivation}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Objectives}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Theoretical Background}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Deep Reinforcement Learning}{1}\protected@file@percent }
\newlabel{deep_rl}{{2.1}{1}}
\citation{franccois2018introduction}
\citation{franccois2018introduction}
\citation{watkins1992q}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Agent takes an action to move to the next state and observe a reward \cite {franccois2018introduction}\relax }}{2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:2}{{1}{2}}
\newlabel{q_update}{{2}{2}}
\citation{lin1993reinforcement}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Policy Network for Super Mario (not to scale)\relax }}{3}\protected@file@percent }
\newlabel{fig:2}{{2}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Target Networks}{3}\protected@file@percent }
\citation{geirhos2018generalisation}
\citation{sutton1996generalization}
\citation{van2017hybrid}
\citation{cobbe2018quantifying}
\citation{ribas2011neural}
\citation{ribas2011neural}
\citation{sutton1999between}
\citation{sutton1999between}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Experience Replay}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Hierarchical Reinforcement Learning}{4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The Hierarchical RL Semi Markov Decision Process \cite {ribas2011neural}\relax }}{4}\protected@file@percent }
\newlabel{fig:4}{{3}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Option-Critic}{4}\protected@file@percent }
\citation{bacon2017option}
\citation{ribas2011neural}
\citation{ribas2011neural}
\citation{vezhnevets2017feudal}
\citation{dayan1993feudal}
\citation{vezhnevets2017feudal}
\citation{vezhnevets2017feudal}
\citation{DBLP:journals/corr/MnihKSGAWR13}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Option-Critic Architecture \cite {ribas2011neural}\relax }}{5}\protected@file@percent }
\newlabel{fig:4}{{4}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Feudal Networks}{5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces FuN Architecure \cite {vezhnevets2017feudal}\relax }}{5}\protected@file@percent }
\newlabel{fig:4}{{5}{5}}
\citation{marioai}
\citation{jython}
\citation{jpype}
\citation{py4j}
\@writefile{toc}{\contentsline {section}{\numberline {3}System Requirements}{6}\protected@file@percent }
\citation{marioai}
\citation{siebert2001kobra}
\@writefile{toc}{\contentsline {section}{\numberline {4}System Implementation}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Overall Architecture}{7}\protected@file@percent }
\citation{pypi}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces System Architecture\relax }}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Reward Function}{8}\protected@file@percent }
\newlabel{reward_function}{{4.2}{8}}
\citation{sutton2011reinforcement}
\citation{white2012bandit}
\citation{mnih2015human}
\citation{maroti2019rbed}
\citation{chuchro2017game}
\citation{gribbon2004novel}
\citation{DBLP:journals/corr/MnihKSGAWR13}
\citation{lample2017playing}
\citation{DBLP:journals/corr/MnihKSGAWR13}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Exploitation vs Exploration}{9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Variants of $\epsilon $ Decay\relax }}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Frame Preprocessing}{9}\protected@file@percent }
\newlabel{frame_preprocessing}{{4.4}{9}}
\citation{braylan2015frame}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Frame Preprocessing Pipeline\relax }}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Frame Stacking and Frame Skipping}{10}\protected@file@percent }
\newlabel{frame_stacking_skipping}{{4.5}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Stacked frames are given as input to the neural network. The dimension of the input thus becomes $84 \times 84 \times 3 \times 4$\relax }}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Custom Levels}{10}\protected@file@percent }
\newlabel{custom_levels}{{4.6}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The agent selects action $a$ to be executed $k$ times in the emulator and observes the $k^{th}$ frame\relax }}{11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Steps with Gap Level\relax }}{11}\protected@file@percent }
\newlabel{fig:steps_gap_level}{{11}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Two Pipes Level\relax }}{11}\protected@file@percent }
\newlabel{fig:}{{12}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Steps with Gap + Two Pipes\relax }}{11}\protected@file@percent }
\newlabel{fig:awesome_image3}{{13}{11}}
\citation{bacon2017option}
\citation{kulkarni2016hierarchical}
\citation{hessel2018rainbow}
\citation{levy2017learning}
\citation{lecun2015deep}
\citation{ramachandran2017searching}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments and Results}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Experiment Design}{12}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Experiments carried out for the project. Progressive indicates whether the model has been pretrained on another level already.\relax }}{12}\protected@file@percent }
\newlabel{tbl:experiments}{{1}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces DQN Hyperparameters}}{13}\protected@file@percent }
\newlabel{tbl:dqn_hyperparams}{{2}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}DQN Agent}{13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Illustration of the CNN used by the DQN Agent (not to scale). Stride is the number of pixels the convolutional filter moves. Kernel is the size of the filter.\relax }}{13}\protected@file@percent }
\newlabel{fig:dqn_cnn}{{14}{13}}
\citation{bacon2017option}
\newlabel{fig:dqn_results_1}{{15a}{14}}
\newlabel{sub@fig:dqn_results_1}{{a}{14}}
\newlabel{fig:dqn_results_2}{{15b}{14}}
\newlabel{sub@fig:dqn_results_2}{{b}{14}}
\newlabel{fig:dqn_results_3}{{15c}{14}}
\newlabel{sub@fig:dqn_results_3}{{c}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces DQN Performance on the three Super Mario levels measured in the average reward per played episode.\relax }}{14}\protected@file@percent }
\newlabel{fig:dqn_results}{{15}{14}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Rewards accumulated by the DQN agent in each level. Final rewards are the total rewards acquired in the final episode of the run. Total rewards is the sum of all rewards of the run.\relax }}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Option-Critic Agent}{14}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The total sum of credits accumulated in each level by the Option Critic agent.\relax }}{14}\protected@file@percent }
\newlabel{tbl:oc_rewards}{{4}{14}}
\newlabel{fig:oc_results_1}{{16a}{15}}
\newlabel{sub@fig:oc_results_1}{{a}{15}}
\newlabel{fig:oc_results_2}{{16b}{15}}
\newlabel{sub@fig:oc_results_2}{{b}{15}}
\newlabel{fig:oc_results_3}{{16c}{15}}
\newlabel{sub@fig:oc_results_3}{{c}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Option Critic Performance on the three Super Mario levels measured in the average reward per played episode.\relax }}{15}\protected@file@percent }
\newlabel{fig:oc_results}{{16}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}FuN Agent}{15}\protected@file@percent }
\newlabel{fig:fun_results_1}{{17a}{15}}
\newlabel{sub@fig:fun_results_1}{{a}{15}}
\newlabel{fig:fun_results_2}{{17b}{15}}
\newlabel{sub@fig:fun_results_2}{{b}{15}}
\newlabel{fig:fun_results_3}{{17c}{15}}
\newlabel{sub@fig:fun_results_3}{{c}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces FuN Performance on the three Super Mario levels measured in the average reward per played episode.\relax }}{15}\protected@file@percent }
\newlabel{fig:fun_results}{{17}{15}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces The total sum of credits accumulated in each level by the FuN agent.\relax }}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Evaluation of Results}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Critical Assessment of Project}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{16}\protected@file@percent }
\bibstyle{ieeetran}
\bibdata{references}
\bibcite{DBLP:journals/corr/MnihKSGAWR13}{1}
\bibcite{alpha_go}{2}
\bibcite{arulkumaran2019alphastar}{3}
\bibcite{hessel2018rainbow}{4}
\bibcite{bellemare2017distributional}{5}
\bibcite{nachum2019does}{6}
\bibcite{puterman2014markov}{7}
\bibcite{franccois2018introduction}{8}
\bibcite{watkins1992q}{9}
\bibcite{lin1993reinforcement}{10}
\bibcite{geirhos2018generalisation}{11}
\bibcite{sutton1996generalization}{12}
\bibcite{van2017hybrid}{13}
\bibcite{cobbe2018quantifying}{14}
\bibcite{ribas2011neural}{15}
\bibcite{sutton1999between}{16}
\bibcite{bacon2017option}{17}
\bibcite{vezhnevets2017feudal}{18}
\bibcite{dayan1993feudal}{19}
\bibcite{marioai}{20}
\bibcite{jython}{21}
\bibcite{jpype}{22}
\bibcite{py4j}{23}
\bibcite{siebert2001kobra}{24}
\bibcite{pypi}{25}
\bibcite{sutton2011reinforcement}{26}
\bibcite{white2012bandit}{27}
\bibcite{mnih2015human}{28}
\bibcite{maroti2019rbed}{29}
\bibcite{chuchro2017game}{30}
\bibcite{gribbon2004novel}{31}
\bibcite{lample2017playing}{32}
\bibcite{braylan2015frame}{33}
\bibcite{kulkarni2016hierarchical}{34}
\bibcite{levy2017learning}{35}
\bibcite{lecun2015deep}{36}
\bibcite{ramachandran2017searching}{37}
