\relax 
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\citation{DBLP:journals/corr/MnihKSGAWR13}
\citation{alpha_go}
\citation{arulkumaran2019alphastar}
\citation{hessel2018rainbow}
\citation{bellemare2017distributional}
\citation{nachum2019does}
\citation{puterman2014markov}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Motivation}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Objectives}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Theoretical Background}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Deep Reinforcement Learning}{1}\protected@file@percent }
\citation{franccois2018introduction}
\citation{franccois2018introduction}
\citation{watkins1992q}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Agent takes an action to move to the next state and observe a reward \cite {franccois2018introduction}\relax }}{2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:2}{{1}{2}}
\newlabel{q_update}{{2}{2}}
\citation{lin1993reinforcement}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Policy Network for Super Mario (not to scale)\relax }}{3}\protected@file@percent }
\newlabel{fig:2}{{2}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Target Networks}{3}\protected@file@percent }
\citation{geirhos2018generalisation}
\citation{sutton1996generalization}
\citation{van2017hybrid}
\citation{cobbe2018quantifying}
\citation{sutton1999between}
\citation{ribas2011neural}
\citation{ribas2011neural}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Policy and Target Networks\relax }}{4}\protected@file@percent }
\newlabel{fig:2}{{3}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Experience Replay}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Hierarchical Reinforcement Learning}{4}\protected@file@percent }
\citation{sutton1999between}
\citation{bacon2017option}
\citation{ribas2011neural}
\citation{ribas2011neural}
\citation{vezhnevets2017feudal}
\citation{dayan1993feudal}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The Hierarchical RL Semi Markov Decision Process \cite {ribas2011neural}\relax }}{5}\protected@file@percent }
\newlabel{fig:3}{{4}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Option-Critic}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Feudal Networks}{5}\protected@file@percent }
\citation{vezhnevets2017feudal}
\citation{vezhnevets2017feudal}
\citation{DBLP:journals/corr/MnihKSGAWR13}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Option-Critic Architecture \cite {ribas2011neural}\relax }}{6}\protected@file@percent }
\newlabel{fig:4}{{5}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces FuN Architecure \cite {vezhnevets2017feudal}\relax }}{6}\protected@file@percent }
\newlabel{fig:4}{{6}{6}}
\citation{marioai}
\citation{jython}
\citation{jpype}
\citation{py4j}
\@writefile{toc}{\contentsline {section}{\numberline {3}System Requirements}{7}\protected@file@percent }
\citation{marioai}
\citation{siebert2001kobra}
\@writefile{toc}{\contentsline {section}{\numberline {4}System Implementation}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Overall Architecture}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Reward Function}{8}\protected@file@percent }
\newlabel{reward_function}{{4.2}{8}}
\citation{pypi}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces System Architecture\relax }}{9}\protected@file@percent }
\citation{sutton2011reinforcement}
\citation{white2012bandit}
\citation{mnih2015human}
\citation{maroti2019rbed}
\citation{chuchro2017game}
\citation{gribbon2004novel}
\citation{DBLP:journals/corr/MnihKSGAWR13}
\citation{lample2017playing}
\citation{DBLP:journals/corr/MnihKSGAWR13}
\citation{braylan2015frame}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Exploitation vs Exploration}{10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Variants of $\epsilon $ Decay\relax }}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Frame Preprocessing}{10}\protected@file@percent }
\newlabel{frame_preprocessing}{{4.4}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Frame Stacking and Frame Skipping}{10}\protected@file@percent }
\newlabel{frame_stacking_skipping}{{4.5}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Frame Preprocessing Pipeline\relax }}{11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Stacked frames are given as input to the neural network. The dimension of the input thus becomes $84 \times 84 \times 3 \times 4$\relax }}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Custom Levels}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments and Results}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Experiment Design}{11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The agent selects action $a$ to be executed $k$ times in the emulator and observes the $k^{th}$ frame\relax }}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}DQN Agent}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Option-Critic Agent}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Feudal Agent}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Evaluation of Results}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Critical Assessment of Project}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{12}\protected@file@percent }
\bibstyle{ieeetran}
\bibdata{references}
\bibcite{DBLP:journals/corr/MnihKSGAWR13}{1}
\bibcite{alpha_go}{2}
\bibcite{arulkumaran2019alphastar}{3}
\bibcite{hessel2018rainbow}{4}
\bibcite{bellemare2017distributional}{5}
\bibcite{nachum2019does}{6}
\bibcite{puterman2014markov}{7}
\bibcite{franccois2018introduction}{8}
\bibcite{watkins1992q}{9}
\bibcite{lin1993reinforcement}{10}
\bibcite{geirhos2018generalisation}{11}
\bibcite{sutton1996generalization}{12}
\bibcite{van2017hybrid}{13}
\bibcite{cobbe2018quantifying}{14}
\bibcite{sutton1999between}{15}
\bibcite{ribas2011neural}{16}
\bibcite{bacon2017option}{17}
\bibcite{vezhnevets2017feudal}{18}
\bibcite{dayan1993feudal}{19}
\bibcite{marioai}{20}
\bibcite{jython}{21}
\bibcite{jpype}{22}
\bibcite{py4j}{23}
\bibcite{siebert2001kobra}{24}
\bibcite{sutton2011reinforcement}{25}
\bibcite{white2012bandit}{26}
\bibcite{mnih2015human}{27}
\bibcite{maroti2019rbed}{28}
\bibcite{chuchro2017game}{29}
\bibcite{gribbon2004novel}{30}
\bibcite{lample2017playing}{31}
\bibcite{braylan2015frame}{32}
