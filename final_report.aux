\relax 
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\citation{DBLP:journals/corr/MnihKSGAWR13}
\citation{alpha_go}
\citation{arulkumaran2019alphastar}
\citation{hessel2018rainbow}
\citation{bellemare2017distributional}
\citation{nachum2019does}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Motivation}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Objectives}{1}\protected@file@percent }
\citation{puterman2014markov}
\citation{franccois2018introduction}
\citation{franccois2018introduction}
\citation{watkins1992q}
\@writefile{toc}{\contentsline {section}{\numberline {2}Literature Review}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Deep Reinforcement Learning}{2}\protected@file@percent }
\newlabel{deep_rl}{{2.1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Agent takes an action to move to the next state and observe a reward \cite {franccois2018introduction}\relax }}{2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:2}{{1}{2}}
\newlabel{q_update}{{2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Policy Network for Super Mario (not to scale)\relax }}{3}\protected@file@percent }
\newlabel{fig:2}{{2}{3}}
\citation{lin1993reinforcement}
\citation{geirhos2018generalisation}
\citation{sutton1996generalization}
\citation{van2017hybrid}
\citation{cobbe2018quantifying}
\citation{ribas2011neural}
\citation{ribas2011neural}
\citation{sutton1999between}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Target Networks}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Experience Replay}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Hierarchical Reinforcement Learning}{4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The Hierarchical RL Semi Markov Decision Process \cite {ribas2011neural}\relax }}{4}\protected@file@percent }
\newlabel{fig:4}{{3}{4}}
\citation{sutton1999between}
\citation{bacon2017option}
\citation{ribas2011neural}
\citation{ribas2011neural}
\citation{vezhnevets2017feudal}
\citation{dayan1993feudal}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Option-Critic}{5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Option-Critic Architecture \cite {ribas2011neural}\relax }}{5}\protected@file@percent }
\newlabel{fig:4}{{4}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Feudal Networks}{5}\protected@file@percent }
\citation{vezhnevets2017feudal}
\citation{vezhnevets2017feudal}
\citation{DBLP:journals/corr/MnihKSGAWR13}
\citation{marioai}
\citation{jython}
\citation{jpype}
\citation{py4j}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces FuN Architecure \cite {vezhnevets2017feudal}\relax }}{6}\protected@file@percent }
\newlabel{fig:4}{{5}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {3}System Requirements}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}System Implementation}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Super Mario Game}{7}\protected@file@percent }
\citation{marioai}
\citation{siebert2001kobra}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Overall Architecture}{8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces System Architecture\relax }}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Reward Function}{8}\protected@file@percent }
\newlabel{reward_function}{{4.3}{8}}
\citation{pypi}
\citation{sutton2011reinforcement}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Exploitation vs Exploration}{9}\protected@file@percent }
\newlabel{exploit_explore}{{4.4}{9}}
\citation{white2012bandit}
\citation{mnih2015human}
\citation{maroti2019rbed}
\citation{chuchro2017game}
\citation{gribbon2004novel}
\citation{DBLP:journals/corr/MnihKSGAWR13}
\citation{lample2017playing}
\citation{DBLP:journals/corr/MnihKSGAWR13}
\citation{braylan2015frame}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Variants of $\epsilon $ Decay\relax }}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Frame Preprocessing}{10}\protected@file@percent }
\newlabel{frame_preprocessing}{{4.5}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Frame Preprocessing Pipeline\relax }}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Frame Stacking and Frame Skipping}{10}\protected@file@percent }
\newlabel{frame_stacking_skipping}{{4.6}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Stacked frames are given as input to the neural network. The dimension of the input thus becomes $84 \times 84 \times 3 \times 4$\relax }}{11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The agent selects action $a$ to be executed $k$ times in the emulator and observes the $k^{th}$ frame\relax }}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Custom Levels}{11}\protected@file@percent }
\newlabel{custom_levels}{{4.7}{11}}
\citation{bacon2017option}
\citation{kulkarni2016hierarchical}
\citation{hessel2018rainbow}
\citation{levy2017learning}
\citation{lecun2015deep}
\citation{ramachandran2017searching}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Steps with Gap Level\relax }}{12}\protected@file@percent }
\newlabel{fig:steps_gap_level}{{11}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Two Pipes Level\relax }}{12}\protected@file@percent }
\newlabel{fig:}{{12}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Steps with Gap + Two Pipes\relax }}{12}\protected@file@percent }
\newlabel{fig:awesome_image3}{{13}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments and Results}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Experiment Design}{12}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Experiments carried out for the project. sequential indicates whether the model has been pretrained on another level already.\relax }}{13}\protected@file@percent }
\newlabel{tbl:experiments}{{1}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}DQN Agent}{13}\protected@file@percent }
\newlabel{dqn_experiment}{{5.2}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Illustration of the CNN used by the DQN Agent (not to scale). Stride is the number of pixels the convolutional filter moves. Kernel is the size of the filter.\relax }}{13}\protected@file@percent }
\newlabel{fig:dqn_cnn}{{14}{13}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces DQN Hyperparameters}}{14}\protected@file@percent }
\newlabel{tbl:dqn_hyperparams}{{2}{14}}
\newlabel{fig:dqn_results_1}{{15a}{14}}
\newlabel{sub@fig:dqn_results_1}{{a}{14}}
\newlabel{fig:dqn_results_2}{{15b}{14}}
\newlabel{sub@fig:dqn_results_2}{{b}{14}}
\newlabel{fig:dqn_results_3}{{15c}{14}}
\newlabel{sub@fig:dqn_results_3}{{c}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces DQN Performance on the three Super Mario levels measured in the average reward per played episode.\relax }}{14}\protected@file@percent }
\newlabel{fig:dqn_results}{{15}{14}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Rewards accumulated by the DQN agent in each level. Final rewards are the total rewards acquired in the final episode of the run. Total rewards is the sum of all rewards of the run.\relax }}{14}\protected@file@percent }
\citation{bacon2017option}
\citation{oc_github}
\citation{fun_github}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Option-Critic Agent}{15}\protected@file@percent }
\newlabel{oc_experiment}{{5.3}{15}}
\newlabel{fig:oc_results_1}{{16a}{15}}
\newlabel{sub@fig:oc_results_1}{{a}{15}}
\newlabel{fig:oc_results_2}{{16b}{15}}
\newlabel{sub@fig:oc_results_2}{{b}{15}}
\newlabel{fig:oc_results_3}{{16c}{15}}
\newlabel{sub@fig:oc_results_3}{{c}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Option Critic Performance on the three Super Mario levels measured in the average reward per played episode.\relax }}{15}\protected@file@percent }
\newlabel{fig:oc_results}{{16}{15}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The total sum of credits accumulated in each level by the Option Critic agent.\relax }}{15}\protected@file@percent }
\newlabel{tbl:oc_rewards}{{4}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}FuN Agent}{15}\protected@file@percent }
\newlabel{fun_experiment}{{5.4}{15}}
\citation{kirkpatrick2017overcoming}
\newlabel{fig:fun_results_1}{{17a}{16}}
\newlabel{sub@fig:fun_results_1}{{a}{16}}
\newlabel{fig:fun_results_2}{{17b}{16}}
\newlabel{sub@fig:fun_results_2}{{b}{16}}
\newlabel{fig:fun_results_3}{{17c}{16}}
\newlabel{sub@fig:fun_results_3}{{c}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces FuN Performance on the three Super Mario levels measured in the average reward per played episode.\relax }}{16}\protected@file@percent }
\newlabel{fig:fun_results}{{17}{16}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces The total sum of credits accumulated in each level by the FuN agent.\relax }}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Evaluation of Results}{16}\protected@file@percent }
\newlabel{fig:all_results_1}{{18a}{17}}
\newlabel{sub@fig:all_results_1}{{a}{17}}
\newlabel{fig:fun_results_2}{{18b}{17}}
\newlabel{sub@fig:fun_results_2}{{b}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Comparing the average rewards per episode for the two levels where models were pretrained.\relax }}{17}\protected@file@percent }
\newlabel{fig:all_results}{{18}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Further Policy Analysis}{17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces The average reward per run and the number of completed episodes. Pearson is a linear correlation coefficient. Spearman is a nonlinear correlation coefficient.\relax }}{17}\protected@file@percent }
\newlabel{fig:good_policy}{{19}{17}}
\citation{chuchro2017game}
\citation{song2016measuring}
\@writefile{toc}{\contentsline {section}{\numberline {6}Critical Assessment of Project}{18}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{19}\protected@file@percent }
\bibstyle{ieeetran}
\bibdata{references}
\bibcite{DBLP:journals/corr/MnihKSGAWR13}{1}
\bibcite{alpha_go}{2}
\bibcite{arulkumaran2019alphastar}{3}
\bibcite{hessel2018rainbow}{4}
\bibcite{bellemare2017distributional}{5}
\bibcite{nachum2019does}{6}
\bibcite{puterman2014markov}{7}
\bibcite{franccois2018introduction}{8}
\bibcite{watkins1992q}{9}
\bibcite{lin1993reinforcement}{10}
\bibcite{geirhos2018generalisation}{11}
\bibcite{sutton1996generalization}{12}
\bibcite{van2017hybrid}{13}
\bibcite{cobbe2018quantifying}{14}
\bibcite{ribas2011neural}{15}
\bibcite{sutton1999between}{16}
\bibcite{bacon2017option}{17}
\bibcite{vezhnevets2017feudal}{18}
\bibcite{dayan1993feudal}{19}
\bibcite{marioai}{20}
\bibcite{jython}{21}
\bibcite{jpype}{22}
\bibcite{py4j}{23}
\bibcite{siebert2001kobra}{24}
\bibcite{pypi}{25}
\bibcite{sutton2011reinforcement}{26}
\bibcite{white2012bandit}{27}
\bibcite{mnih2015human}{28}
\bibcite{maroti2019rbed}{29}
\bibcite{chuchro2017game}{30}
\bibcite{gribbon2004novel}{31}
\bibcite{lample2017playing}{32}
\bibcite{braylan2015frame}{33}
\bibcite{kulkarni2016hierarchical}{34}
\bibcite{levy2017learning}{35}
\bibcite{lecun2015deep}{36}
\bibcite{ramachandran2017searching}{37}
\bibcite{oc_github}{38}
\bibcite{fun_github}{39}
\bibcite{kirkpatrick2017overcoming}{40}
\bibcite{song2016measuring}{41}
